% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/rhierMnlRwMixture_rcpp.R
\name{rhierMnlRwMixture}
\alias{rhierMnlRwMixture}
\title{Bayesian Multinomial Logit Model with HART Prior}
\usage{
rhierMnlRwMixture(Data, Prior, Mcmc, r_verbose = TRUE)
}
\arguments{
\item{Data}{A list containing:
\itemize{
\item \code{p}: Number of choice alternatives (integer).
\item \code{lgtdata}: A list of length \code{nlgt}. Each element \code{lgtdata[[i]]} must be a list with:
\itemize{
\item \code{y}: \verb{n_i x 1} vector of multinomial outcomes (1 to \code{p}).
\item \code{X}: \verb{(n_i * p) x nvar} matrix of alternative-specific attributes.
}
\item \code{Z} (optional): \verb{nlgt x nz} matrix of observed characteristics for each unit.
Should NOT contain an intercept and should be centered.
}}

\item{Prior}{A list containing prior parameters:
\itemize{
\item \code{ncomp}: Number of mixture components (required).
\item \code{a} (optional): \verb{ncomp x 1} vector of Dirichlet prior parameters for mixture weights \code{pvec} (default: \code{rep(5, ncomp)}).
\item \code{deltabar} (optional): \verb{nz * nvar x 1} prior mean for \code{vec(Delta)} (default: 0). Ignored if HART is used.
\item \code{Ad} (optional): Prior precision matrix for \code{vec(Delta)} (default: \code{0.01 * I}). Ignored if HART is used.
\item \code{mubar} (optional): \verb{nvar x 1} prior mean vector for mixture component means (default: 0 if unrestricted, 2 if restricted).
\item \code{Amu} (optional): Prior precision for mixture component means (default: 0.01 if unrestricted, 0.1 if restricted).
\item \code{nu} (optional): Degrees of freedom for IW prior on component \code{Sigma} (default: \code{nvar+3} if unrestricted, \code{nvar+15} if restricted).
\item \code{V} (optional): Location matrix for IW prior on component \code{Sigma} (default: \code{nu * I} or scaled based on restriction).
\item \code{SignRes} (optional): \verb{nvar x 1} vector of sign restrictions. Must contain values of 0, -1, or 1. The value 0 means no restriction, -1 ensures the coefficient is negative, and 1 ensures the coefficient is positive. For example, \code{SignRes = c(0,1,-1)} means the first coefficient is unconstrained, the second will be positive, and the third will be negative. Default: \code{rep(0, nvar)}.
\item \code{bart} (optional): List of parameters for the HART prior. If specified, this models the representative consumer \eqn{\Delta(Z)} as a scaled sum-of-trees factor model. See Details.
}}

\item{Mcmc}{A list containing MCMC parameters:
\itemize{
\item \code{R}: Number of MCMC iterations (required).
\item \code{keep} (optional): Thinning parameter (default: 1).
\item \code{nprint} (optional): Print progress every \code{nprint} draws (default: 100, 0 for none).
\item \code{s} (optional): RW Metropolis scaling parameter (default: \code{2.93 / sqrt(nvar)}).
\item \code{w} (optional): Fractional likelihood weighting parameter (default: 0.1).
}}

\item{r_verbose}{Logical. Print startup messages? Default TRUE.}
}
\value{
A list containing:
\itemize{
\item \code{Deltadraw}: If \code{Z} provided and \code{bart=NULL}, \verb{(R/keep) x (nz * nvar)} matrix of \code{vec(Delta)} draws.
\item \code{betadraw}: \verb{nlgt x nvar x (R/keep)} array of unit-level \code{beta_i} draws.
\item \code{nmix}: List containing mixture draws with components:
\itemize{
\item \code{probdraw}: \verb{(R/keep) x ncomp} matrix of mixture component probabilities.
\item \code{zdraw}: \verb{(R/keep) x nlgt} matrix of component assignments for each unit.
\item \code{compdraw}: \code{(R/keep)} list of \code{ncomp} lists. \code{compdraw[[r]][[j]] = list(mu, rooti)} contains the draw of \eqn{\mu_j} and \eqn{\Sigma_j^{-1/2}} for component \code{j} at kept draw \code{r}.
}
\item \code{loglike}: \verb{(R/keep) x 1} vector of log-likelihood values at kept draws.
\item \code{SignRes}: \verb{nvar x 1} vector of sign restrictions used.
\item \code{bart_trees}: If HART used, list containing tree structures and related parameters.
}
}
\description{
\code{rhierMnlRwMixture} implements an MCMC algorithm for a Bayesian multinomial
logit model with a Hierarchical Additive Regression Trees (HART) prior. HART
is a hierarchical nonparametric prior that allows for flexible modeling of the
representative consumer as a function of potentially many observed characteristics.
}
\details{
\subsection{Model Specification}{

\eqn{y_i \sim MNL(X_i, \beta_i)} for unit \(i = 1, \ldots, nlgt\).
The unit-level coefficients (part-worths) \eqn{\beta_i} are modeled as:
\deqn{\beta_i = \Delta(Z_i) + u_i}
where \eqn{\Delta(Z_i)} is the \emph{representative consumer} component, which depends on observed characteristics \eqn{Z_i}, and \eqn{u_i} is the unobserved heterogeneity component.

The representative consumer component is specified as:
\itemize{
\item If \code{Z} is provided and \code{Prior$bart} is \code{NULL}: \eqn{\Delta(Z_i) = Z_i \Delta} where \eqn{\Delta} is an \verb{nz x nvar} matrix (linear hierarchical model).
\item If \code{Z} is provided and \code{Prior$bart} is a list: \eqn{\Delta(Z_i)} is modeled with a HART prior (scaled sum-of-trees factor model).
\item If \code{Z} is \code{NULL}: \eqn{\Delta(Z_i) = 0}.
}

With \code{ncomp = 1} (currently required), the unobserved heterogeneity component follows:
\deqn{u_i \sim N(\mu_1, \Sigma_1)}
}

\subsection{Prior Specifications}{
\itemize{
\item \strong{Mixture weights}: \eqn{pvec \sim Dirichlet(a)}
\item \strong{Linear model}: \eqn{\delta = vec(\Delta) \sim N(deltabar, A_d^{-1})}
\item \strong{Mixture component means}: \eqn{\mu_j \sim N(mubar, \Sigma_j \otimes Amu^{-1})} (covariance scaled by \eqn{\Sigma_j})
\item \strong{Mixture component covariance}: \eqn{\Sigma_j \sim IW(\nu, V)}
\item \strong{HART model}: A sum-of-trees prior is placed on each factor of the scaled sum-of-trees model (see HART details below).
}
}

\subsection{HART Prior Details}{

If \code{Prior$bart} is a list, it specifies a HART prior for the representative consumer \eqn{\Delta(Z)}.
This replaces the conventional linear hierarchical specification. The HART prior models the representative
consumer using a scaled vector of \code{nvar} sum-of-trees models.

\strong{HART Parameters} (defaults used if not specified in \code{Prior$bart}):
\itemize{
\item \code{num_trees}: Number of trees H in each sum-of-trees model (default: 200).
\item \code{power}, \code{base}: Parameters for the tree structure prior. The probability of a node at depth \code{q} splitting is \eqn{\alpha(1+q)^{-\beta}}, where \code{base}=\eqn{\alpha} and \code{power}=\eqn{\beta}. Defaults are \code{base=0.95}, \code{power=2}, which strongly favors shallow trees.
\item \code{tau}: Parameter controlling the prior variance of terminal leaf coefficients. The default is \eqn{\tau = 1/\sqrt{H}} where \eqn{\lambda_{dhg} \sim N(0, \tau^2)} for terminal leaf coefficients.
\item \code{numcut}: Number of grid points for proposing splitting rules for continuous variables (default: 100).
\item \code{sparse}: If \code{TRUE}, use the Dirichlet HART prior to induce sparsity in variable selection (default: \code{FALSE}).
}

\strong{Dirichlet HART} (\code{sparse = TRUE}): The Dirichlet HART model augments the HART prior to induce sparsity in variable selection, following Linero (2018). Instead of uniform probability for selecting splitting variables, the selection probabilities \eqn{\tau = (\tau^{(1)}, \ldots, \tau^{(K)})} are given a sparse Dirichlet prior: \eqn{(\tau^{(1)}, \ldots, \tau^{(K)}) \sim Dirichlet(\theta/K, \ldots, \theta/K)}, where K is the number of characteristics. The concentration parameter \eqn{\theta} is given a hierarchical prior: \eqn{\theta/(\theta+\rho) \sim Beta(a,b)}.
\itemize{
\item \code{a}, \code{b}: Shape parameters for the Beta hyperprior. The default (\verb{a=0.5, b=1}) induces sparsity where few variables have high selection probabilities.
\item \code{rho}: Parameter influencing sparsity. Default is the number of characteristics K. Reducing rho below K encourages greater sparsity.
\item \code{theta}: When used, sets Dirichlet concentration parameter without additional hyper-prior (default: 0.0).
\item \code{burn}: Number of internal burn-in iterations for the Dirichlet HART sampler before variable selection is allowed (default: 100).
}
}

\subsection{Sign Restrictions}{

If \code{SignRes[k]} is non-zero, the k-th coefficient \eqn{\beta_{ik}} is modeled as
\deqn{\beta_{ik} = SignRes[k] \cdot \exp(\beta^*_{ik}).}
The \code{betadraw} output contains the draws for \eqn{\beta_{ik}} (with the restriction applied).
The \code{nmix} output contains draws for the \emph{unrestricted} mixture components.

\strong{Note:} Care should be taken when selecting priors on any sign restricted coefficients.
}
}
\note{
Currently, the mixture component is not implemented. Please use
\code{ncomp = 1} in the Prior specification.
}
\references{
Chipman, Hugh A., Edward I. George, and Robert E. McCulloch (2010). "BART: Bayesian Additive Regression Trees." Annals of Applied Statistics 4.1.

Linero, Antonio R. (2018). "Bayesian regression trees for high-dimensional prediction and variable selection." Journal of the American Statistical Association 113.522, pp. 626-636.

Rossi, Peter E., Greg M. Allenby, and Robert McCulloch (2009). Bayesian Statistics and Marketing. Reprint. Wiley Series in Probability and Statistics. Chichester: Wiley.

Rossi, Peter (2023). bayesm: Bayesian Inference for Marketing/Micro-Econometrics. Comprehensive R Archive Network.

Wiemann, Thomas (2025). "Personalization with HART." Working paper.
}
\seealso{
\code{\link[=predict.rhierMnlRwMixture]{predict.rhierMnlRwMixture()}}
}
\author{
Peter Rossi (original bayesm code), Thomas Wiemann (HART modifications).
}
